# Efficient Distributed Inference on GPT
## Abstract
Efficient distributed inference of large-scale machine learning models presents significant challenges. This project compares three methods to enhance inference efficiency: Knowledge Distillation, Tensor Partitioning, and Model Parallelism. Using the GPT-2 Medium model and the Penn Treebank dataset, we implemented each method under consistent conditions.

## Contact:
* Cheng-An Hsieh(chengan2)
* Ben Chiang
* Ling-En Huang


